{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Numpy arrays are multidimensional arrays (ndarrays) which are very widely used\n",
    "##for basically 2 reasons\n",
    "##(i) have built in methods to execute commonly used linear algebra operations\n",
    "##note that in this context, nd arrays typically represent vectors, matricies, or tensors from math though\n",
    "## its not always that case that the items in side an ndarray are #s\n",
    "X = np.eye(3)\n",
    "print(X)\n",
    "X.transpose()\n",
    "np.linalg.cholesky(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1.]\n",
      " [1. 1. 1.]\n",
      " [1. 1. 1.]]\n",
      "[[2. 1. 1.]\n",
      " [1. 2. 1.]\n",
      " [1. 1. 2.]]\n",
      "[[4. 4. 4.]\n",
      " [4. 4. 4.]\n",
      " [4. 4. 4.]]\n"
     ]
    }
   ],
   "source": [
    "## (ii) Numpy arrays support broadcastin of many common operations\n",
    "# broadcasting is the applic of operation to each element in a collection of items \n",
    "# when that operation is applied to the collection\n",
    "X = np.eye(3) #make a ndarray\n",
    "Y = np.ones([3,3])\n",
    "print(Y)\n",
    "print(X+Y) #elementwise addition\n",
    "print((2*Y)**2) #square each elment in matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Access elements in an ndarray with typical indexing, rows then columns for matricies:\n",
    "X = np.eye(3)\n",
    "print(X[0,1]) #returns the element in the 0th row, 1st col\n",
    "## Can use slice indexing to access ranges of elements, though be careful with upper bound on this index\n",
    "print(X[0:2,0:2]) ##returns element in rows 0 through 1, col 0 through 2 :()\n",
    "\n",
    "X[:,1] ##returns all emeents in column 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Grading goes faster if we can easily see your results. Make sure that you turn in a copy of your notebook file with all cells executed]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Practice with numpy\n",
    "\n",
    "In this question we will be practicing with indexing numpy arrays.  Given the numpy matrix (ndarray) numbers, defined below, your first task (part A) will be to describe in words what each code cell will output (ideally without running the code cell first...).  In part B, you will be asked to write code that returns certain elements of this numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = np.array([[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Describe expressions.\n",
    "\n",
    "After each expression there is an empty comment. Use this comment to describe what the expression returns. The first one is given to you as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers[0,0]\n",
    "\n",
    "## [Example answer] This returns the first element of the first row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers[1,2:4]\n",
    "\n",
    "## Returns the 3rd and 4th element of the 2nd row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers[:,3]\n",
    "\n",
    "## returns all the elements of the last (4th) column in each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers ** 2\n",
    "\n",
    "## squares every elment in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers[0,:] + numbers[1,:]\n",
    "\n",
    "## add the first 2 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers[:,0] * numbers[:,1]\n",
    "\n",
    "##  multiplies all the elments of the first column by the 2nd column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers[:,0].dot(numbers[:,1])\n",
    "\n",
    "## multiply the first 2 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "print(2+30+90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.sum(axis=0)\n",
    "\n",
    "## sum every column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers.sum(axis=1)\n",
    "\n",
    "## sum every row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Write expressions\n",
    "\n",
    "In this section each block will contain a comment describing a result, you will write code to generate that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = numpy.array([[1, 2, 3, 4],\n",
    "                       [5, 6, 7, 8],\n",
    "                       [9, 10, 11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return 10\n",
    "\n",
    "print(numbers[2:,1])\n",
    "sum(numbers[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 9])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## return array([5, 9])\n",
    "\n",
    "numbers[1:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return array([[7, 8], [11, 12]])\n",
    "\n",
    "numbers[1:3,2:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[14, 16],\n",
       "       [22, 24]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## return array([[14, 16], [22, 24]])\n",
    "\n",
    "numbers[1:3,2:4]*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.        , 0.69314718]), array([1.60943791, 1.79175947]))"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## return array([[0., 0.69314718], [1.60943791, 1.79175947]])\n",
    "## (hint: what is log(2)?)\n",
    "\n",
    "np.log(numbers[0,:2]),np.log(numbers[1,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## return the sum of the squares of the first row\n",
    "## (hint: use dot())\n",
    "# print(1 + 4 + 9 + 16)\n",
    "# numbers[0,:1]\n",
    "# sum()\n",
    "numbers[:,0].dot(numbers[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([225, 324, 441, 576], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## return the square of the sums of the columns\n",
    "numbers.sum(axis=0)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 6., 7., 8.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## return the mean of the columns (numpy.mean() works like numpy.sum())\n",
    "numbers.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Covariance, correlation, regression\n",
    "\n",
    "A. Write a function called `generate_linear` that takes four arguments: a sample size ($n$), a slope ($m$), an intercept ($b$), and an error scale ($s$). It should return a numpy array with $n$ rows and 2 columns. The first column ($X$) should be $n$ values from a normal distribution with mean 0 and standard deviation 1. The second column ($Y$) should be $mx + b$ plus $s$ times a second array of $n$ values from a standard normal distribution (mean 0, std 1). (See notes for an example)\n",
    "\n",
    "Test this by generating a dataset with $n=100, m=-0.5, b=1.0, s=0.2$. Plot this dataset using `pyplot.scatter()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer here\n",
    "\n",
    "def generate_linear(n,m,b,s):\n",
    "    x = np.random.normal(0,1,size=n) \n",
    "    error = np.random.normal(0, 1, size=n)\n",
    "    y = x * m + b + (error*s)\n",
    "    a = np.column_stack([x,y])\n",
    "    #print(a.shape)\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot\n",
    "n=100\n",
    "m=-0.5\n",
    "b=1.0\n",
    "s=0.2\n",
    "a = generate_linear(n,m,b,s)\n",
    "pyplot.scatter(a[:,0],a[:,1])\n",
    "pyplot.plot()\n",
    "#print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. Write a function called `regression` that takes a two-column (ie $X, Y$) numpy array and returns a tuple containing the covariance of the two variables, the variance of $X$, and the estimated slope parameter of a linear regression $\\hat{m}$ ($cov(X,Y)/var(X)$). You may use code from class notes. Test this function by displaying the result for the dataset you created in part A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Answer here\n",
    "\n",
    "def regression(a):\n",
    "    cov_matrix = np.cov(a[:,0],a[:,1])\n",
    "    cov = cov_matrix[1,0]\n",
    "    varX= cov_matrix[0,0]\n",
    "    m = cov / varX\n",
    "    tup = (cov, varX, m)\n",
    "    return tup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.46901680012099184, 0.953193371214833, -0.49204790369370255)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regression(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Write code to generate five datasets for each combination of the following parameter values: $m \\in \\{1, 0.25, 0.05\\}, n \\in \\{10, 100, 1000\\}, s \\in \\{0.01, 0.1, 1.0\\}$. (Use $b$ = 0.) For each dataset print the parameter settings using a formatted string and the estimated regression slope $\\hat{m}$. Do not show floating point numbers with more than four decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m:  1 n:  10 s:  0.01 b: 0 m̂:  0.9924 Diff:  0.0076\n",
      "m:  1 n:  10 s:  0.01 b: 0 m̂:  0.9953 Diff:  0.0047\n",
      "m:  1 n:  10 s:  0.01 b: 0 m̂:  0.9948 Diff:  0.0052\n",
      "m:  1 n:  10 s:  0.01 b: 0 m̂:  0.9987 Diff:  0.0013\n",
      "m:  1 n:  10 s:  0.01 b: 0 m̂:  0.9979 Diff:  0.0021\n",
      "m:  1 n:  10 s:  0.1 b: 0 m̂:  1.0224 Diff:  0.0224\n",
      "m:  1 n:  10 s:  0.1 b: 0 m̂:  1.1031 Diff:  0.1031\n",
      "m:  1 n:  10 s:  0.1 b: 0 m̂:  0.9901 Diff:  0.0099\n",
      "m:  1 n:  10 s:  0.1 b: 0 m̂:  0.9970 Diff:  0.0030\n",
      "m:  1 n:  10 s:  0.1 b: 0 m̂:  1.0018 Diff:  0.0018\n",
      "m:  1 n:  10 s:  1.0 b: 0 m̂:  0.9398 Diff:  0.0602\n",
      "m:  1 n:  10 s:  1.0 b: 0 m̂:  0.4930 Diff:  0.5070\n",
      "m:  1 n:  10 s:  1.0 b: 0 m̂:  1.4500 Diff:  0.4500\n",
      "m:  1 n:  10 s:  1.0 b: 0 m̂:  1.4537 Diff:  0.4537\n",
      "m:  1 n:  10 s:  1.0 b: 0 m̂:  1.4068 Diff:  0.4068\n",
      "m:  1 n:  100 s:  0.01 b: 0 m̂:  1.0003 Diff:  0.0003\n",
      "m:  1 n:  100 s:  0.01 b: 0 m̂:  1.0011 Diff:  0.0011\n",
      "m:  1 n:  100 s:  0.01 b: 0 m̂:  1.0021 Diff:  0.0021\n",
      "m:  1 n:  100 s:  0.01 b: 0 m̂:  0.9996 Diff:  0.0004\n",
      "m:  1 n:  100 s:  0.01 b: 0 m̂:  0.9999 Diff:  0.0001\n",
      "m:  1 n:  100 s:  0.1 b: 0 m̂:  1.0016 Diff:  0.0016\n",
      "m:  1 n:  100 s:  0.1 b: 0 m̂:  0.9927 Diff:  0.0073\n",
      "m:  1 n:  100 s:  0.1 b: 0 m̂:  0.9952 Diff:  0.0048\n",
      "m:  1 n:  100 s:  0.1 b: 0 m̂:  0.9990 Diff:  0.0010\n",
      "m:  1 n:  100 s:  0.1 b: 0 m̂:  0.9861 Diff:  0.0139\n",
      "m:  1 n:  100 s:  1.0 b: 0 m̂:  0.9282 Diff:  0.0718\n",
      "m:  1 n:  100 s:  1.0 b: 0 m̂:  1.0973 Diff:  0.0973\n",
      "m:  1 n:  100 s:  1.0 b: 0 m̂:  1.1098 Diff:  0.1098\n",
      "m:  1 n:  100 s:  1.0 b: 0 m̂:  0.8882 Diff:  0.1118\n",
      "m:  1 n:  100 s:  1.0 b: 0 m̂:  1.0418 Diff:  0.0418\n",
      "m:  1 n:  1000 s:  0.01 b: 0 m̂:  0.9997 Diff:  0.0003\n",
      "m:  1 n:  1000 s:  0.01 b: 0 m̂:  1.0002 Diff:  0.0002\n",
      "m:  1 n:  1000 s:  0.01 b: 0 m̂:  1.0001 Diff:  0.0001\n",
      "m:  1 n:  1000 s:  0.01 b: 0 m̂:  1.0002 Diff:  0.0002\n",
      "m:  1 n:  1000 s:  0.01 b: 0 m̂:  1.0000 Diff:  0.0000\n",
      "m:  1 n:  1000 s:  0.1 b: 0 m̂:  0.9997 Diff:  0.0003\n",
      "m:  1 n:  1000 s:  0.1 b: 0 m̂:  1.0061 Diff:  0.0061\n",
      "m:  1 n:  1000 s:  0.1 b: 0 m̂:  1.0017 Diff:  0.0017\n",
      "m:  1 n:  1000 s:  0.1 b: 0 m̂:  0.9969 Diff:  0.0031\n",
      "m:  1 n:  1000 s:  0.1 b: 0 m̂:  0.9960 Diff:  0.0040\n",
      "m:  1 n:  1000 s:  1.0 b: 0 m̂:  0.9787 Diff:  0.0213\n",
      "m:  1 n:  1000 s:  1.0 b: 0 m̂:  1.0090 Diff:  0.0090\n",
      "m:  1 n:  1000 s:  1.0 b: 0 m̂:  1.0353 Diff:  0.0353\n",
      "m:  1 n:  1000 s:  1.0 b: 0 m̂:  0.9826 Diff:  0.0174\n",
      "m:  1 n:  1000 s:  1.0 b: 0 m̂:  1.0188 Diff:  0.0188\n",
      "m:  0.25 n:  10 s:  0.01 b: 0 m̂:  0.2491 Diff:  0.0009\n",
      "m:  0.25 n:  10 s:  0.01 b: 0 m̂:  0.2478 Diff:  0.0022\n",
      "m:  0.25 n:  10 s:  0.01 b: 0 m̂:  0.2506 Diff:  0.0006\n",
      "m:  0.25 n:  10 s:  0.01 b: 0 m̂:  0.2465 Diff:  0.0035\n",
      "m:  0.25 n:  10 s:  0.01 b: 0 m̂:  0.2502 Diff:  0.0002\n",
      "m:  0.25 n:  10 s:  0.1 b: 0 m̂:  0.2682 Diff:  0.0182\n",
      "m:  0.25 n:  10 s:  0.1 b: 0 m̂:  0.2689 Diff:  0.0189\n",
      "m:  0.25 n:  10 s:  0.1 b: 0 m̂:  0.2540 Diff:  0.0040\n",
      "m:  0.25 n:  10 s:  0.1 b: 0 m̂:  0.2448 Diff:  0.0052\n",
      "m:  0.25 n:  10 s:  0.1 b: 0 m̂:  0.2170 Diff:  0.0330\n",
      "m:  0.25 n:  10 s:  1.0 b: 0 m̂:  0.2113 Diff:  0.0387\n",
      "m:  0.25 n:  10 s:  1.0 b: 0 m̂:  0.2942 Diff:  0.0442\n",
      "m:  0.25 n:  10 s:  1.0 b: 0 m̂:  -0.0746 Diff:  0.3246\n",
      "m:  0.25 n:  10 s:  1.0 b: 0 m̂:  -0.8214 Diff:  1.0714\n",
      "m:  0.25 n:  10 s:  1.0 b: 0 m̂:  0.1872 Diff:  0.0628\n",
      "m:  0.25 n:  100 s:  0.01 b: 0 m̂:  0.2500 Diff:  0.0000\n",
      "m:  0.25 n:  100 s:  0.01 b: 0 m̂:  0.2508 Diff:  0.0008\n",
      "m:  0.25 n:  100 s:  0.01 b: 0 m̂:  0.2499 Diff:  0.0001\n",
      "m:  0.25 n:  100 s:  0.01 b: 0 m̂:  0.2505 Diff:  0.0005\n",
      "m:  0.25 n:  100 s:  0.01 b: 0 m̂:  0.2502 Diff:  0.0002\n",
      "m:  0.25 n:  100 s:  0.1 b: 0 m̂:  0.2650 Diff:  0.0150\n",
      "m:  0.25 n:  100 s:  0.1 b: 0 m̂:  0.2495 Diff:  0.0005\n",
      "m:  0.25 n:  100 s:  0.1 b: 0 m̂:  0.2518 Diff:  0.0018\n",
      "m:  0.25 n:  100 s:  0.1 b: 0 m̂:  0.2371 Diff:  0.0129\n",
      "m:  0.25 n:  100 s:  0.1 b: 0 m̂:  0.2371 Diff:  0.0129\n",
      "m:  0.25 n:  100 s:  1.0 b: 0 m̂:  0.1596 Diff:  0.0904\n",
      "m:  0.25 n:  100 s:  1.0 b: 0 m̂:  0.1680 Diff:  0.0820\n",
      "m:  0.25 n:  100 s:  1.0 b: 0 m̂:  0.2258 Diff:  0.0242\n",
      "m:  0.25 n:  100 s:  1.0 b: 0 m̂:  0.3654 Diff:  0.1154\n",
      "m:  0.25 n:  100 s:  1.0 b: 0 m̂:  0.2478 Diff:  0.0022\n",
      "m:  0.25 n:  1000 s:  0.01 b: 0 m̂:  0.2505 Diff:  0.0005\n",
      "m:  0.25 n:  1000 s:  0.01 b: 0 m̂:  0.2500 Diff:  0.0000\n",
      "m:  0.25 n:  1000 s:  0.01 b: 0 m̂:  0.2496 Diff:  0.0004\n",
      "m:  0.25 n:  1000 s:  0.01 b: 0 m̂:  0.2502 Diff:  0.0002\n",
      "m:  0.25 n:  1000 s:  0.01 b: 0 m̂:  0.2501 Diff:  0.0001\n",
      "m:  0.25 n:  1000 s:  0.1 b: 0 m̂:  0.2498 Diff:  0.0002\n",
      "m:  0.25 n:  1000 s:  0.1 b: 0 m̂:  0.2498 Diff:  0.0002\n",
      "m:  0.25 n:  1000 s:  0.1 b: 0 m̂:  0.2497 Diff:  0.0003\n",
      "m:  0.25 n:  1000 s:  0.1 b: 0 m̂:  0.2536 Diff:  0.0036\n",
      "m:  0.25 n:  1000 s:  0.1 b: 0 m̂:  0.2504 Diff:  0.0004\n",
      "m:  0.25 n:  1000 s:  1.0 b: 0 m̂:  0.3074 Diff:  0.0574\n",
      "m:  0.25 n:  1000 s:  1.0 b: 0 m̂:  0.2436 Diff:  0.0064\n",
      "m:  0.25 n:  1000 s:  1.0 b: 0 m̂:  0.2287 Diff:  0.0213\n",
      "m:  0.25 n:  1000 s:  1.0 b: 0 m̂:  0.2594 Diff:  0.0094\n",
      "m:  0.25 n:  1000 s:  1.0 b: 0 m̂:  0.2865 Diff:  0.0365\n",
      "m:  0.05 n:  10 s:  0.01 b: 0 m̂:  0.0510 Diff:  0.0010\n",
      "m:  0.05 n:  10 s:  0.01 b: 0 m̂:  0.0478 Diff:  0.0022\n",
      "m:  0.05 n:  10 s:  0.01 b: 0 m̂:  0.0493 Diff:  0.0007\n",
      "m:  0.05 n:  10 s:  0.01 b: 0 m̂:  0.0499 Diff:  0.0001\n",
      "m:  0.05 n:  10 s:  0.01 b: 0 m̂:  0.0415 Diff:  0.0085\n",
      "m:  0.05 n:  10 s:  0.1 b: 0 m̂:  0.1343 Diff:  0.0843\n",
      "m:  0.05 n:  10 s:  0.1 b: 0 m̂:  0.0377 Diff:  0.0123\n",
      "m:  0.05 n:  10 s:  0.1 b: 0 m̂:  -0.0090 Diff:  0.0590\n",
      "m:  0.05 n:  10 s:  0.1 b: 0 m̂:  0.0224 Diff:  0.0276\n",
      "m:  0.05 n:  10 s:  0.1 b: 0 m̂:  0.0400 Diff:  0.0100\n",
      "m:  0.05 n:  10 s:  1.0 b: 0 m̂:  0.0194 Diff:  0.0306\n",
      "m:  0.05 n:  10 s:  1.0 b: 0 m̂:  -0.0859 Diff:  0.1359\n",
      "m:  0.05 n:  10 s:  1.0 b: 0 m̂:  0.0445 Diff:  0.0055\n",
      "m:  0.05 n:  10 s:  1.0 b: 0 m̂:  0.1080 Diff:  0.0580\n",
      "m:  0.05 n:  10 s:  1.0 b: 0 m̂:  0.0162 Diff:  0.0338\n",
      "m:  0.05 n:  100 s:  0.01 b: 0 m̂:  0.0473 Diff:  0.0027\n",
      "m:  0.05 n:  100 s:  0.01 b: 0 m̂:  0.0492 Diff:  0.0008\n",
      "m:  0.05 n:  100 s:  0.01 b: 0 m̂:  0.0486 Diff:  0.0014\n",
      "m:  0.05 n:  100 s:  0.01 b: 0 m̂:  0.0487 Diff:  0.0013\n",
      "m:  0.05 n:  100 s:  0.01 b: 0 m̂:  0.0486 Diff:  0.0014\n",
      "m:  0.05 n:  100 s:  0.1 b: 0 m̂:  0.0479 Diff:  0.0021\n",
      "m:  0.05 n:  100 s:  0.1 b: 0 m̂:  0.0539 Diff:  0.0039\n",
      "m:  0.05 n:  100 s:  0.1 b: 0 m̂:  0.0468 Diff:  0.0032\n",
      "m:  0.05 n:  100 s:  0.1 b: 0 m̂:  0.0626 Diff:  0.0126\n",
      "m:  0.05 n:  100 s:  0.1 b: 0 m̂:  0.0533 Diff:  0.0033\n",
      "m:  0.05 n:  100 s:  1.0 b: 0 m̂:  -0.1087 Diff:  0.1587\n",
      "m:  0.05 n:  100 s:  1.0 b: 0 m̂:  0.1946 Diff:  0.1446\n",
      "m:  0.05 n:  100 s:  1.0 b: 0 m̂:  0.1748 Diff:  0.1248\n",
      "m:  0.05 n:  100 s:  1.0 b: 0 m̂:  0.0898 Diff:  0.0398\n",
      "m:  0.05 n:  100 s:  1.0 b: 0 m̂:  0.0698 Diff:  0.0198\n",
      "m:  0.05 n:  1000 s:  0.01 b: 0 m̂:  0.0494 Diff:  0.0006\n",
      "m:  0.05 n:  1000 s:  0.01 b: 0 m̂:  0.0498 Diff:  0.0002\n",
      "m:  0.05 n:  1000 s:  0.01 b: 0 m̂:  0.0500 Diff:  0.0000\n",
      "m:  0.05 n:  1000 s:  0.01 b: 0 m̂:  0.0500 Diff:  0.0000\n",
      "m:  0.05 n:  1000 s:  0.01 b: 0 m̂:  0.0498 Diff:  0.0002\n",
      "m:  0.05 n:  1000 s:  0.1 b: 0 m̂:  0.0512 Diff:  0.0012\n",
      "m:  0.05 n:  1000 s:  0.1 b: 0 m̂:  0.0507 Diff:  0.0007\n",
      "m:  0.05 n:  1000 s:  0.1 b: 0 m̂:  0.0491 Diff:  0.0009\n",
      "m:  0.05 n:  1000 s:  0.1 b: 0 m̂:  0.0471 Diff:  0.0029\n",
      "m:  0.05 n:  1000 s:  0.1 b: 0 m̂:  0.0524 Diff:  0.0024\n",
      "m:  0.05 n:  1000 s:  1.0 b: 0 m̂:  0.0431 Diff:  0.0069\n",
      "m:  0.05 n:  1000 s:  1.0 b: 0 m̂:  0.0553 Diff:  0.0053\n",
      "m:  0.05 n:  1000 s:  1.0 b: 0 m̂:  0.0763 Diff:  0.0263\n",
      "m:  0.05 n:  1000 s:  1.0 b: 0 m̂:  -0.0310 Diff:  0.0810\n",
      "m:  0.05 n:  1000 s:  1.0 b: 0 m̂:  0.0106 Diff:  0.0394\n"
     ]
    }
   ],
   "source": [
    "## Answer here\n",
    "b = 0\n",
    "for m in [1,0.25,0.05]:\n",
    "    for n in [10,100,1000]:\n",
    "        for s in [0.01,0.1,1.0]:\n",
    "            for i in range(5):\n",
    "                a = generate_linear(n,m,b,s)\n",
    "                r= regression(a)\n",
    "                print ('m: ',m,'n: ',n,'s: ',s, 'b: 0', 'm̂: ', \"%.4f\" % r[2], \"Diff: \", \"%.4f\" % abs(r[2]-m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D. Are you able to estimate the slope parameter reliably in the presence of noisy data? How is the quality of your estimate affected by sample size $n$, \"effect size\" (ie $m$), and degree of noise (ie $s$)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here:\n",
    "\n",
    "The greater the sample size the more data there exists and thus the more reliable our data is because we can reduce the influence of outliers (data sets with higher erros). The greater the degree of noise the less reliable our data becomes because there is a greater chance for a data point to be an outlier (have a higher error deviation ) The effect size (m) or slope in this case doesn't neccessarily impact the reliability of the values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Spurious correlations\n",
    "\n",
    "If you have enough data sets, you can find connections between surprising variables through random chance. The website [Spurious Correlations](http://www.tylervigen.com/spurious-correlations) by Tyler Vigen contains many of these. Here is one such comparison, the average number of pounds of cheese consumed per year per person and the number of people who died after getting tangled in the sheets of their bed.\n",
    "\n",
    "Write code to calculate the correlation between these variables, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedsheet_tanglings = np.array([327, 456, 509, 497, 506, 573, 661, 741, 809, 717])\n",
    "cheese_per_capita = np.array([29.8, 30.1, 30.5, 30.6, 31.3, 31.7, 32.6, 33.1, 32.7, 32.8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate and print the Pearson correlation coefficient between these two variables here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson Correlation Coefficient:  0.9448891469341408\n"
     ]
    }
   ],
   "source": [
    "a = np.corrcoef(bedsheet_tanglings,cheese_per_capita)\n",
    "print(\"Pearson Correlation Coefficient: \", a[0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are you convinced that cheese consumption is predictive of bedsheet tangling deaths? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here: Since the value is very close to one there does appear to be a correlation between both of these variables (bedsheet tanglings and cheese per capita). But logically speaking this is most likely just by chance as correlation doesn't neccessarily mean causation. The site this data comes from appears to gather hundreds of sets of data and see if there ever is a correlation with a relatively small sample size. Ofcoarse of the billions of unrelated variables that exist, there will often be a strong correlation between 2 unrelated variables just by chance and not by causation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
